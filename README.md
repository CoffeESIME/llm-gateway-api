# LLM Gateway API

API Gateway inteligente que enruta automÃ¡ticamente peticiones entre modelos locales (Ollama) y modelos en la nube (Google Gemini) basÃ¡ndose en el parÃ¡metro de privacidad del usuario.

## ğŸ¯ CaracterÃ­sticas

- **Routing Inteligente**: SelecciÃ³n automÃ¡tica de modelo basada en tipo de tarea y modo de privacidad
- **Multi-Modal**: Soporte para texto, visiÃ³n, OCR y embeddings
- **Compatible OpenAI**: Formato de API compatible con OpenAI Chat Completions
- **Local + Cloud**: Usa modelos locales Ollama para privacidad estricta, Gemini para flexibilidad
- **FastAPI**: API moderna con documentaciÃ³n automÃ¡tica (Swagger)

## ğŸ“‹ Requisitos Previos

1. **Python 3.10+**
2. **Ollama** instalado y corriendo con los siguientes modelos:
   - `CognitiveComputations/dolphin-mistral-nemo:latest`
   - `qwen3-vl:8b`
   - `deepseek-ocr:3b`
   - `nomic-embed-text:latest`
3. **Google Gemini API Key** (para modo flexible)

## ğŸš€ InstalaciÃ³n

### 1. Clonar e instalar dependencias

```bash
# Navegar al directorio
cd llm-endpoints

# Crear entorno virtual (recomendado)
python -m venv venv
.\venv\Scripts\activate  # Windows
# source venv/bin/activate  # Linux/Mac

# Instalar dependencias
pip install -r requirements.txt
```

### 2. Configurar variables de entorno

```bash
# Copiar el archivo de ejemplo
copy .env.example .env

# Editar .env con tu editor y agregar tu GEMINI_API_KEY
# AsegÃºrate de reemplazar 'your_gemini_api_key_here' con tu API key real
```

> [!IMPORTANT]
> **Debes crear el archivo `.env`** copiando `.env.example` y agregando tu `GEMINI_API_KEY` real.
> Sin esta API key, solo podrÃ¡s usar `privacy_mode: "strict"` (modelos locales).

### 3. Verificar Ollama

```bash
# Verificar que Ollama estÃ¡ corriendo
ollama list

# DeberÃ­a mostrar los modelos instalados
# Si faltan modelos, descargarlos:
# ollama pull CognitiveComputations/dolphin-mistral-nemo:latest
# ollama pull qwen3-vl:8b
# ollama pull deepseek-ocr:3b
# ollama pull nomic-embed-text:latest
```

### 4. Iniciar el servidor

```bash
# OpciÃ³n 1: Usar uvicorn directamente
uvicorn main:app --reload --port 8765

# OpciÃ³n 2: Ejecutar el script main.py
python main.py
```

El servidor estarÃ¡ disponible en: `http://localhost:8765`

## ğŸ“š DocumentaciÃ³n Interactiva

Una vez iniciado el servidor:
- **Swagger UI**: http://localhost:8765/docs
- **ReDoc**: http://localhost:8765/redoc

## ğŸ”§ Uso

### Estructura de la PeticiÃ³n

```json
{
  "task": "chat | vision | ocr | embedding",
  "privacy_mode": "strict | flexible",
  "messages": [
    {"role": "user", "content": "..."}
  ],
  "temperature": 0.7,
  "max_tokens": 500
}
```

**ParÃ¡metros principales:**
- `task`: Tipo de tarea a realizar
- `privacy_mode`: 
  - `strict`: Usa modelos locales (Ollama)
  - `flexible`: Usa modelos cloud (Gemini)

### Ejemplo 1: Chat Privado (Local)

```bash
curl -X POST http://localhost:8765/v1/chat/completions \
-H "Content-Type: application/json" \
-d '{
  "task": "chat",
  "privacy_mode": "strict",
  "messages": [{"role": "user", "content": "Resume este texto confidencial..."}],
  "temperature": 0.7,
  "max_tokens": 500
}'
```

**Modelo usado**: `ollama/CognitiveComputations/dolphin-mistral-nemo:latest`

### Ejemplo 2: AnÃ¡lisis de Imagen (Cloud)

```bash
curl -X POST http://localhost:8765/v1/chat/completions \
-H "Content-Type: application/json" \
-d '{
  "task": "vision",
  "privacy_mode": "flexible",
  "messages": [
    {
      "role": "user",
      "content": [
        {"type": "text", "text": "Â¿QuÃ© lugar es este?"},
        {"type": "image_url", "image_url": {"url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/320px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg"}}
      ]
    }
  ]
}'
```

**Modelo usado**: `gemini/gemini-2.5-pro`

### Ejemplo 3: OCR Local

```bash
curl -X POST http://localhost:8765/v1/chat/completions \
-H "Content-Type: application/json" \
-d '{
  "task": "ocr",
  "privacy_mode": "strict",
  "messages": [
    {
      "role": "user",
      "content": [
        {"type": "text", "text": "Extrae el texto de esta imagen"},
        {"type": "image_url", "image_url": {"url": "URL_DE_TU_IMAGEN"}}
      ]
    }
  ]
}'
```

**Modelo usado**: `ollama/deepseek-ocr:3b`

## ğŸ—ºï¸ Routing de Modelos

| Task | Privacy: Strict (Local) | Privacy: Flexible (Cloud) |
|------|------------------------|---------------------------|
| **chat** | `ollama/dolphin-mistral-nemo:latest` | `gemini/gemini-2.5-flash` |
| **vision** | `ollama/qwen3-vl:8b` | `gemini/gemini-2.5-pro` |
| **ocr** | `ollama/deepseek-ocr:3b` | `gemini/gemini-2.5-flash` |
| **embedding** | `ollama/nomic-embed-text:latest` | `ollama/nomic-embed-text:latest` |

## ğŸ“ Estructura del Proyecto

```
llm-endpoints/
â”œâ”€â”€ main.py                 # AplicaciÃ³n FastAPI principal
â”œâ”€â”€ config.py              # ConfiguraciÃ³n y MODEL_ROUTER
â”œâ”€â”€ requirements.txt       # Dependencias
â”œâ”€â”€ .env.example          # Template de variables de entorno
â”œâ”€â”€ routers/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ chat.py           # Endpoint de chat completions
â”œâ”€â”€ schemas/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ requests.py       # Pydantic schemas de request
â”‚   â””â”€â”€ responses.py      # Pydantic schemas de response
â””â”€â”€ services/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ router.py         # LÃ³gica de routing de modelos
    â””â”€â”€ llm_client.py     # Cliente LiteLLM
```

## ğŸ” Health Check

```bash
curl http://localhost:8765/health
```

Respuesta esperada:
```json
{"status": "ok"}
```

## ğŸ“Š Listar Modelos Disponibles

```bash
curl http://localhost:8765/v1/models
```

## ğŸ› Troubleshooting

### Error: "Modelo no encontrado"
- Verificar que Ollama estÃ¡ corriendo: `ollama list`
- Descargar el modelo faltante: `ollama pull <modelo>`

### Error: "Error de autenticaciÃ³n"
- Verificar que `GEMINI_API_KEY` estÃ¡ configurada en `.env`
- Verificar que la API key es vÃ¡lida

### Error: "Connection refused"
- Verificar que Ollama estÃ¡ corriendo en `http://localhost:11434`
- Cambiar `OLLAMA_BASE_URL` en `.env` si es necesario

## ğŸ“ Licencia

MIT

## ğŸ¤ Contribuciones

Las contribuciones son bienvenidas. Por favor abre un issue o pull request.
